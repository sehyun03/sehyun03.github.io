<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Sehyun Hwang</title> <meta name="author" content="Sehyun Hwang"/> <meta name="description" content="Sehyun's personal webpage. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="sehyun"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/vs.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üó∫Ô∏è</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://sehyun03.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/zenburn.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%73%65%68%79%75%6E%30%33%30@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=GW4KY8IAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/sehyun03" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/sehyun-hwang-864690219" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/cv_sehyun.pdf" target="_blank" rel="noopener noreferrer">Curriculum Vitae</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Sehyun</span> Hwang </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_sehyun_v4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_sehyun_v4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_sehyun_v4-1400.webp"></source> <img src="/assets/img/prof_sehyun_v4.jpeg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_sehyun_v4.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> </div> </div> <div class="clearfix"> <p>I am a research scientist in the Video Recommendations Ranking Modeling team at <a href="https://www.meta.com/" target="_blank" rel="noopener noreferrer">Meta</a>. I received my Ph.D. from POSTECH, where I was advised by Prof. <a href="https://suhakwak.github.io/" target="_blank" rel="noopener noreferrer">Suha Kwak</a> in the <a href="http://cvlab.postech.ac.kr/lab/" target="_blank" rel="noopener noreferrer">Computer Vision Lab at POSTECH</a>.</p> <p>During my Ph.D., I worked on active learning, domain adaptation, and debiasing in machine learning and computer vision, with a focus on data-efficient and robust learning. </p> </div> <div class="news"> <h2>News</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width:16%">Nov 3, 2025</th> <td> üíº I joined Video Recommendations Ranking Modeling team at <a href="https://about.meta.com/" target="_blank" rel="noopener noreferrer">Meta</a> as a research scientist. </td> </tr> <tr> <th scope="row" style="width:16%">Aug 1, 2025</th> <td> üíº I joined <a href="http://cvlab.dgist.ac.kr/" target="_blank" rel="noopener noreferrer">Computer Vision Lab</a> at DGIST as a postdoctoral researcher. </td> </tr> <tr> <th scope="row" style="width:16%">Jul 22, 2025</th> <td> üìö A <a href="https://yehogwon.github.io/csq/" target="_blank" rel="noopener noreferrer">paper</a> about designing novel query for active learning is accepted to <a href="https://jmlr.org/tmlr/" target="_blank" rel="noopener noreferrer">TMLR</a>. </td> </tr> <tr> <th scope="row" style="width:16%">Mar 1, 2025</th> <td> üíº I joined <a href="http://cvlab.postech.ac.kr/" target="_blank" rel="noopener noreferrer">Computer Vision Lab</a> at POSTECH as a postdoctoral researcher. </td> </tr> </table> </div> </div> <div class="education"> <h2>Education</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Sep, 2018 - Feb, 2025</th> <td> <a href="https://postech.ac.kr/eng/" target="_blank" rel="noopener noreferrer">Pohang University of Science and Technology (POSTECH)</a>, Pohang, South Korea <br> Integrated M.S./Ph.D. student in <a href="https://cse.postech.ac.kr/" target="_blank" rel="noopener noreferrer">Computer Science and Engineering</a> <br> Advisor: Prof. <a href="https://suhakwak.github.io/" target="_blank" rel="noopener noreferrer">Suha Kwak</a> </td> </tr> <tr> <th scope="row">Mar, 2014 - Aug, 2018</th> <td> <a href="https://dgist.ac.kr/en/" target="_blank" rel="noopener noreferrer">Daegu Gyeongbuk Institute of Science and Technology (DGIST)</a>, Daegu, South Korea <br> B.S. in School of Undergraduate Studies </td> </tr> </table> </div> </div> <div class="experience"> <h2>Experience</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Nov, 2025 - Present</th> <td> Video Recommendations Ranking Modeling team at <a href="https://about.meta.com/" target="_blank" rel="noopener noreferrer">Meta</a>, Menlo Park, CA, USA <br> <em>Research Scientist</em> <ul> <li>Working on large-scale video recommendation models for Facebook Reels.</li> </ul> </td> </tr> <tr> <th scope="row">Aug, 2025 - Sep, 2025</th> <td> <a href="https://cvlab.dgist.ac.kr/" target="_blank" rel="noopener noreferrer">Computer Vision Lab</a> at <a href="https://www.dgist.ac.kr/" target="_blank" rel="noopener noreferrer">DGIST</a>, Daegu, South Korea <br> <em>Postdoctoral Researcher</em> <ul> <li>Advisor: Prof. Sunghoon Im</li> <li>Participate in research projects about designing diffusion policy for embodied AI control.</li> </ul> </td> </tr> <tr> <th scope="row">Mar, 2025 - May, 2025</th> <td> <a href="http://cvlab.postech.ac.kr/lab/" target="_blank" rel="noopener noreferrer">Computer Vision Lab</a> at <a href="https://postech.ac.kr/" target="_blank" rel="noopener noreferrer">POSTECH</a>, Pohang, South Korea <br> <em>Postdoctoral Researcher</em> <ul> <li>Advisor: Prof. Suha Kwak</li> <li>Participate in research projects about scalable acive learning and efficient image tokenization.</li> </ul> </td> </tr> <tr> <th scope="row">July, 2024 - Sep, 2024</th> <td> <a href="https://www.microsoft.com/en-us/research/group/biomedical-imaging/" target="_blank" rel="noopener noreferrer">Biomedical Imaging Group</a> at Microsoft Research Cambridge, Cambridge, UK <br> <em>Research Intern</em> <ul> <li>Mentor: Maximilian Ilse</li> <li>Participate in research projects about tube segmentation on chest X-ray.</li> </ul> </td> </tr> </table> </div> </div> <div class="publications"> <h2>Publications</h2> <p>* indicates equal contribution.</p> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gwon2025enhancing-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gwon2025enhancing-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gwon2025enhancing-1400.webp"></source> <img src="/assets/img/gwon2025enhancing.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Gwon2025active" class="col-sm-8"> <div class="title">Enhancing Cost Efficiency in Active Learning with Candidate Set Query</div> <div class="author"> Yeho Gwon*,¬† <em>Sehyun Hwang*</em>,¬†Hoyoung Kim,¬†Jungseul Ok,¬† and Suha Kwak </div> <div class="periodical"> <em>Transactions on Machine Learning Research (<b>TMLR</b>),</em> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2502.06209" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://github.com/yehogwon/csq-al" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>This paper introduces a cost-efficient active learning (AL) framework for classification, featuring a novel query design called candidate set query. Unlike traditional AL queries requiring the oracle to examine all possible classes, our method narrows down the set of candidate classes likely to include the ground-truth class, significantly reducing the search space and labeling cost. Moreover, we leverage conformal prediction to dynamically generate small yet reliable candidate sets, adapting to model enhancement over successive AL rounds. To this end, we introduce an acquisition function designed to prioritize data points that offer high information gain at lower cost. Empirical evaluations on CIFAR-10, CIFAR-100, and ImageNet64x64 demonstrate the effectiveness and scalability of our framework. Notably, it reduces labeling cost by 42% on ImageNet64x64.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/kim2024active-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/kim2024active-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/kim2024active-1400.webp"></source> <img src="/assets/img/kim2024active.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kim2024active" class="col-sm-8"> <div class="title">Active Label Correction for Semantic Segmentation with Foundation Models</div> <div class="author"> Hoyoung Kim,¬† <em>Sehyun Hwang</em>,¬†Suha Kwak,¬† and Jungseul Ok (*equal contribution) </div> <div class="periodical"> <em>International Conference on Machine Learning (<b>ICML</b>),</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2403.10820" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://github.com/ml-postech/active-label-correction" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Training and validating models for semantic segmentation require datasets with pixel-wise annotations, which are notoriously labor-intensive. Although useful priors such as foundation models or crowdsourced datasets are available, they are error-prone. We hence propose an effective framework of active label correction (ALC) based on a design of correction query to rectify pseudo labels of pixels, which in turn is more annotator-friendly than the standard one inquiring to classify a pixel directly according to our theoretical analysis and user study. Specifically, leveraging foundation models providing useful zero-shot predictions on pseudo labels and superpixels, our method comprises two key techniques: (i) an annotator-friendly design of correction query with the pseudo labels, and (ii) an acquisition function looking ahead label expansions based on the superpixels. Experimental results on PASCAL, Cityscapes, and Kvasir-SEG datasets demonstrate the effectiveness of our ALC framework, outperforming prior methods for active semantic segmentation and label correction. Notably, utilizing our method, we obtained a revised dataset of PASCAL by rectifying errors in 2.6 million pixels in PASCAL dataset.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/lee2024extreme-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/lee2024extreme-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/lee2024extreme-1400.webp"></source> <img src="/assets/img/lee2024extreme.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lee2023extreme" class="col-sm-8"> <div class="title">Extreme Point Supervised Instance Segmentation</div> <div class="author"> Hyeonjun Lee,¬† <em>Sehyun Hwang</em>,¬† and Suha Kwak </div> <div class="periodical"> <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2405.20729" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>This paper introduces a new weakly supervised learning approach for instance segmentation using extreme points, i.e., the topmost, leftmost, bottommost, and rightmost points of an object. Although these points are readily available in the modern bounding box annotation process and offer strong clues for precise segmentation, they have received less attention in the literature. Motivated by this, our study explores extreme point supervised instance segmentation to further enhance performance at the same annotation cost with box-supervised methods. Our work considers extreme points as a part of the true instance mask and propagates them to identify potential foreground and background points, which are all together used for training a pseudo label generator. Then pseudo labels given by the generator are in turn used for supervised learning of our final model. Our model generates high-quality masks, particularly when the target object is separated into multiple parts, where previous box-supervised methods often fail. On three public benchmarks, our method significantly outperforms existing box-supervised methods, further narrowing the gap with its fully supervised counterpart.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hwang2023active-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hwang2023active-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hwang2023active-1400.webp"></source> <img src="/assets/img/hwang2023active.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hwang2023active" class="col-sm-8"> <div class="title">Active Learning for Semantic Segmentation with Multi-class Label Query</div> <div class="author"> <em>Sehyun Hwang</em>,¬†Sohyun Lee,¬†Hoyoung Kim,¬†Minhyeon Oh,¬†Jungseul Ok,¬† and Suha Kwak </div> <div class="periodical"> <em>Conference on Neural Information Processing Systems (<b>NeurIPS</b>),</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2309.09319" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://github.com/sehyun03/MulActSeg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>This paper proposes a new active learning method for semantic segmentation. The core of our method lies in a new annotation query design. It samples informative local image regions (e.g., superpixels), and for each of such regions, asks an oracle for a multi-hot vector indicating all classes existing in the region. This multi-class labeling strategy is substantially more efficient than existing ones like segmentation, polygon, and even dominant class labeling in terms of annotation time per click. However, it introduces the class ambiguity issue in training since it assigns partial labels (i.e., a set of candidate classes) to individual pixels. We thus propose a new algorithm for learning semantic segmentation while disambiguating the partial labels in two stages. In the first stage, it trains a segmentation model directly with the partial labels through two new loss functions motivated by partial label learning and multiple instance learning. In the second stage, it disambiguates the partial labels by generating pixel-wise pseudo labels, which are used for supervised learning of the model. Equipped with a new acquisition function dedicated to the multi-class labeling, our method outperformed previous work on Cityscapes and PASCAL VOC 2012 while spending less annotation cost.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/kim2023adaptive-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/kim2023adaptive-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/kim2023adaptive-1400.webp"></source> <img src="/assets/img/kim2023adaptive.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kim2022adaptive" class="col-sm-8"> <div class="title">Adaptive Superpixel for Active Learning in Semantic Segmentation</div> <div class="author"> Hoyoung Kim,¬†Minhyeon Oh,¬† <em>Sehyun Hwang</em>,¬†Suha Kwak,¬† and Jungseul Ok </div> <div class="periodical"> <em>International Conference on Computer Vision (<b>ICCV</b>),</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2303.16817" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://github.com/ml-postech/adaptive-superpixel-for-active-learning-in-semantic-segmentation" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Learning semantic segmentation requires pixel-wise annotations, which can be time-consuming and expensive. To reduce the annotation cost, we propose a superpixel-based active learning (AL) framework, which collects a dominant label per superpixel instead. To be specific, it consists of adaptive superpixel and sieving mechanisms, fully dedicated to AL. At each round of AL, we adaptively merge neighboring pixels of similar learned features into superpixels. We then query a selected subset of these superpixels using an acquisition function assuming no uniform superpixel size. This approach is more efficient than existing methods, which rely only on innate features such as RGB color and assume uniform superpixel sizes. Obtaining a dominant label per superpixel drastically reduces annotators‚Äô burden as it requires fewer clicks. However, it inevitably introduces noisy annotations due to mismatches between superpixel and ground truth segmentation. To address this issue, we further devise a sieving mechanism that identifies and excludes potentially noisy annotations from learning. Our experiments on both Cityscapes and PASCAL VOC datasets demonstrate the efficacy of adaptive superpixel and sieving mechanisms.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hwang2023combating2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hwang2023combating2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hwang2023combating2-1400.webp"></source> <img src="/assets/img/hwang2023combating2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hwang2022combating" class="col-sm-8"> <div class="title">Combating label distribution shift for active domain adaptation</div> <div class="author"> <em>Sehyun Hwang</em>,¬†Sohyun Lee,¬†Sungyeon Kim,¬†Jungseul Ok,¬† and Suha Kwak </div> <div class="periodical"> <em>European Conference on Computer Vision (<b>ECCV</b>),</em> 2022 </div> <span style="color:red"><b>Qualcomm Innovation Fellowship Winner, Gold Prize at IPIU paper Award</b></span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2208.06604" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://github.com/sehyun03/ADA-label-distribution-matching" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>We consider the problem of active domain adaptation (ADA) to unlabeled target data, of which subset is actively selected and labeled given a budget constraint. Inspired by recent analysis on a critical issue from label distribution mismatch between source and target in domain adaptation, we devise a method that addresses the issue for the first time in ADA. At its heart lies a novel sampling strategy, which seeks target data that best approximate the entire target distribution as well as being representative, diverse, and uncertain. The sampled target data are then used not only for supervised learning but also for matching label distributions of source and target domains, leading to remarkable performance improvement. On four public benchmarks, our method substantially outperforms existing methods in every adaptation scenario.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/kim2022debias2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/kim2022debias2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/kim2022debias2-1400.webp"></source> <img src="/assets/img/kim2022debias2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kim2022debias" class="col-sm-8"> <div class="title">Learning debiased classifier with biased committee</div> <div class="author"> Nayeong Kim,¬† <em>Sehyun Hwang</em>,¬†Sungsoo Ahn,¬†Jaesik Park,¬† and Suha Kwak </div> <div class="periodical"> <em>Conference on Neural Information Processing Systems (<b>NeurIPS</b>),</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2206.10843" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://github.com/Nayeong-V-Kim/LWBC" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Neural networks are prone to be biased towards spurious correlations between classes and latent attributes exhibited in a major portion of training data, which ruins their generalization capability. We propose a new method for training debiased classifiers with no spurious attribute label. The key idea is to employ a committee of classifiers as an auxiliary module that identifies bias-conflicting data, i.e., data without spurious correlation, and assigns large weights to them when training the main classifier. The committee is learned as a bootstrapped ensemble so that a majority of its classifiers are biased as well as being diverse, and intentionally fail to predict classes of bias-conflicting data accordingly. The consensus within the committee on prediction difficulty thus provides a reliable cue for identifying and weighting bias-conflicting data. Moreover, the committee is also trained with knowledge transferred from the main classifier so that it gradually becomes debiased along with the main classifier and emphasizes more difficult data as training progresses. On five real-world datasets, our method outperforms prior arts using no spurious attribute label like ours and even surpasses those relying on bias labels occasionally.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/kim2022learning-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/kim2022learning-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/kim2022learning-1400.webp"></source> <img src="/assets/img/publication_preview/kim2022learning.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kim2022learning" class="col-sm-8"> <div class="title">Learning to Detect Semantic Boundaries with Image-Level Class Labels</div> <div class="author"> Namyup Kim*,¬† <em>Sehyun Hwang*</em>,¬† and Suha Kwak (*equal contribution) </div> <div class="periodical"> <em>International Journal of Computer Vision (<b>IJCV</b>),</em> 2022 </div> <span style="color:red"><b>Honorable Mention @ Samsung HumanTech Paper Award</b></span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2212.07579" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>This paper presents the first attempt to learn semantic boundary detection using image-level class labels as supervision. Our method starts by estimating coarse areas of object classes through attentions drawn by an image classification network. Since boundaries will locate somewhere between such areas of different classes, our task is formulated as a multiple instance learning (MIL) problem, where pixels on a line segment connecting areas of two different classes are regarded as a bag of boundary candidates. Moreover, we design a new neural network architecture that can learn to estimate semantic boundaries reliably even with uncertain supervision given by the MIL strategy. Our network is used to generate pseudo semantic boundary labels of training images, which are in turn used to train fully supervised models. The final model trained with our pseudo labels achieves an outstanding performance on the SBD dataset, where it is as competitive as some of previous arts trained with stronger supervision.</p> </div> </div> </div> </li> </ol> </div> <div class="honors"> <h2>Honors and Awards</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <td> <strong>BK21 Best Paper Award, POSTECH CSE</strong> <ul> <li>(2025) Best Paper Award - <em>Active Label Correction for Semantic Segmentation with Foundation Models</em> </li> <li>(2024) Best Paper Award - <em>Adaptive Superpixel for Active Learning in Semantic Segmentation</em> </li> <li>(2023) Excellence Award - <em>Learning debiased classifier with biased committee</em> </li> <li>(2023) Excellence Award - <em>Combating label distribution shift for active domain adaptation</em> </li> </ul> <strong>POSTECHIAN Fellowship Award (2023)</strong> <ul> <li>Winner ($5,000)</li> </ul> <strong>Qualcomm Innovation Fellowship South Korea (2022)</strong> <ul> <li>Winner ($3,000) - <em>Combating label distribution shift for active domain adaptation</em> </li> </ul> <strong>IPIU Best Paper Award, IPIU, (2022)</strong> <ul> <li>Gold Prize - <em>Combating label distribution shift for active domain adaptation</em> </li> </ul> <strong>The 26th HumanTech Paper Award, Samsung Electronics Co., Ltd. (2020)</strong> <ul> <li>The Honorable Mention ($3,000) - <em>Learning to Detect Semantic Boundaries with Image‚ÄëLevel Class Labels</em> </li> </ul> </td> </tr> </table> </div> </div> <div class="services"> <h2>Professional Services</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <td> <strong>Reviewer</strong> <ul> <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</li> <li>International Journal of Computer Vision (<strong>IJCV</strong>)</li> <li>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>): 2023-2025</li> <li>International Conference on Learning Representations (<strong>ICLR</strong>): 2025</li> <li>European Conference on Computer Vision (<strong>ECCV</strong>): 2024</li> <li>International Conference on Machine Learning (<strong>ICML</strong>): 2024</li> <li>Asian Conference on Computer Vision (<strong>ACCV</strong> 2022,) 2024</li> <li>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>): 2022-2023</li> <li>International Conference on Computer Vision (<strong>ICCV</strong>): 2023</li> <li>Winter Conference on Applications of Computer Vision (<strong>WACV</strong>): 2022</li> </ul> </td> </tr> </table> </div> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%73%65%68%79%75%6E%30%33%30@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=GW4KY8IAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/sehyun03" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/sehyun-hwang-864690219" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Sehyun Hwang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>